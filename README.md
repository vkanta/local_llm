This  the first attempt to have a  local llm configured with vs-code 
you need to get the ollama tar file from the site .
set it up as a service 
as an exampe 
i have uploaded a sample service file 
you need to install the continue plug in if you get the offline vsx file make sure that you update after the install 
download your immages off line and place them in the llm folder as described in the service file 

